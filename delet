import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
from std_msgs.msg import Float32  # Assuming bbox_center_x is published as a Float32

class ObjectCenteringNode(Node):
    def __init__(self):
        super().__init__('object_centering')
        self.publisher_ = self.create_publisher(Twist, 'cmd_vel', 10)
        self.subscription = self.create_subscription(
            Float32, 
            'bbox_center_x', 
            self.listener_callback, 
            10
        )
        self.image_width = 640  # Example image width
        self.bbox_center_x = None  # Will be updated by the subscriber

    def listener_callback(self, msg):
        self.bbox_center_x = msg.data
        self.update_velocity()

    def update_velocity(self):
        if self.bbox_center_x is None:
            return

        twist = Twist()
        error_x = self.bbox_center_x - (self.image_width / 2)

        # Gain factor, adjust as necessary
        k = 0.005  
        twist.angular.z = -error_x * k

        # Stop the robot if the error is small to avoid oscillation
        if abs(error_x) > 50:
            twist.linear.x = 0.1
        else:
            twist.linear.x = 0

        self.publisher_.publish(twist)

def main(args=None):
    rclpy.init(args=args)
    object_centering_node = ObjectCenteringNode()
    try:
        rclpy.spin(object_centering_node)
    except KeyboardInterrupt:
        pass
    finally:
        object_centering_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()






camera:
  ros__parameters:
    camera_name: "camera"
    camera_namespace: "camera"
    serial_no: ""
    usb_port_id: ""
    device_type: ""
    config_file: ""
    json_file_path: ""
    initial_reset: false
    accelerate_gpu_with_glsl: false
    rosbag_filename: ""
    log_level: "info"
    output: "screen"
    enable_color: true
    rgb_camera:
      color_profile: "0,0,0"
      color_format: "RGB8"
      enable_auto_exposure: true
    enable_depth: true
    enable_infra: false
    enable_infra1: false
    enable_infra2: false
    depth_module:
      depth_profile: "0,0,0"
      depth_format: "Z16"
      infra_profile: "0,0,0"
      infra_format: "RGB8"
      infra1_format: "Y8"
      infra2_format: "Y8"
      exposure: 8500
      gain: 16
      hdr_enabled: false
      enable_auto_exposure: true
      exposure_1: 7500
      gain_1: 16
      exposure_2: 1
      gain_2: 16
    enable_sync: false
    enable_rgbd: false
    enable_gyro: false
    enable_accel: false
    gyro_fps: 0
    accel_fps: 0
    unite_imu_method: 0
    clip_distance: -2.0
    angular_velocity_cov: 0.01
    linear_accel_cov: 0.01
    diagnostics_period: 0.0
    publish_tf: true
    tf_publish_rate: 0.0
    pointcloud:
      enable: false
      stream_filter: 2
      stream_index_filter: 0
      ordered_pc: false
      allow_no_texture_points: false
    align_depth:
      enable: false
    colorizer:
      enable: false
    decimation_filter:
      enable: false
    spatial_filter:
      enable: false
    temporal_filter:
      enable: false
    disparity_filter:
      enable: false
    hole_filling_filter:
      enable: false
    hdr_merge:
      enable: false
    wait_for_device_timeout: -1.0
    reconnect_timeout: 6.0






this the argument that i want in yaml file 

configurable_parameters = [{'name': 'camera_name',                  'default': 'camera', 'description': 'camera unique name'},
                           {'name': 'camera_namespace',             'default': 'camera', 'description': 'namespace for camera'},
                           {'name': 'serial_no',                    'default': "''", 'description': 'choose device by serial number'},
                           {'name': 'usb_port_id',                  'default': "''", 'description': 'choose device by usb port id'},
                           {'name': 'device_type',                  'default': "''", 'description': 'choose device by type'},
                           {'name': 'config_file',                  'default': "''", 'description': 'yaml config file'},
                           {'name': 'json_file_path',               'default': "''", 'description': 'allows advanced configuration'},
                           {'name': 'initial_reset',                'default': 'false', 'description': "''"},
                           {'name': 'accelerate_gpu_with_glsl',     'default': "false", 'description': 'enable GPU acceleration with GLSL'},
                           {'name': 'rosbag_filename',              'default': "''", 'description': 'A realsense bagfile to run from as a device'},
                           {'name': 'log_level',                    'default': 'info', 'description': 'debug log level [DEBUG|INFO|WARN|ERROR|FATAL]'},
                           {'name': 'output',                       'default': 'screen', 'description': 'pipe node output [screen|log]'},
                           {'name': 'enable_color',                 'default': 'true', 'description': 'enable color stream'},
                           {'name': 'rgb_camera.color_profile',     'default': '0,0,0', 'description': 'color stream profile'},
                           {'name': 'rgb_camera.color_format',      'default': 'RGB8', 'description': 'color stream format'},
                           {'name': 'rgb_camera.enable_auto_exposure', 'default': 'true', 'description': 'enable/disable auto exposure for color image'},
                           {'name': 'enable_depth',                 'default': 'true', 'description': 'enable depth stream'},
                           {'name': 'enable_infra',                 'default': 'false', 'description': 'enable infra0 stream'},
                           {'name': 'enable_infra1',                'default': 'false', 'description': 'enable infra1 stream'},
                           {'name': 'enable_infra2',                'default': 'false', 'description': 'enable infra2 stream'},
                           {'name': 'depth_module.depth_profile',   'default': '0,0,0', 'description': 'depth stream profile'},
                           {'name': 'depth_module.depth_format',    'default': 'Z16', 'description': 'depth stream format'},
                           {'name': 'depth_module.infra_profile',   'default': '0,0,0', 'description': 'infra streams (0/1/2) profile'},
                           {'name': 'depth_module.infra_format',    'default': 'RGB8', 'description': 'infra0 stream format'},
                           {'name': 'depth_module.infra1_format',   'default': 'Y8', 'description': 'infra1 stream format'},
                           {'name': 'depth_module.infra2_format',   'default': 'Y8', 'description': 'infra2 stream format'},
                           {'name': 'depth_module.exposure',        'default': '8500', 'description': 'Depth module manual exposure value'},
                           {'name': 'depth_module.gain',            'default': '16', 'description': 'Depth module manual gain value'},
                           {'name': 'depth_module.hdr_enabled',     'default': 'false', 'description': 'Depth module hdr enablement flag. Used for hdr_merge filter'},
                           {'name': 'depth_module.enable_auto_exposure', 'default': 'true', 'description': 'enable/disable auto exposure for depth image'},
                           {'name': 'depth_module.exposure.1',      'default': '7500', 'description': 'Depth module first exposure value. Used for hdr_merge filter'},
                           {'name': 'depth_module.gain.1',          'default': '16', 'description': 'Depth module first gain value. Used for hdr_merge filter'},
                           {'name': 'depth_module.exposure.2',      'default': '1', 'description': 'Depth module second exposure value. Used for hdr_merge filter'},
                           {'name': 'depth_module.gain.2',          'default': '16', 'description': 'Depth module second gain value. Used for hdr_merge filter'},
                           {'name': 'enable_sync',                  'default': 'false', 'description': "'enable sync mode'"},
                           {'name': 'enable_rgbd',                  'default': 'false', 'description': "'enable rgbd topic'"},
                           {'name': 'enable_gyro',                  'default': 'false', 'description': "'enable gyro stream'"},
                           {'name': 'enable_accel',                 'default': 'false', 'description': "'enable accel stream'"},
                           {'name': 'gyro_fps',                     'default': '0', 'description': "''"},
                           {'name': 'accel_fps',                    'default': '0', 'description': "''"},
                           {'name': 'unite_imu_method',             'default': "0", 'description': '[0-None, 1-copy, 2-linear_interpolation]'},
                           {'name': 'clip_distance',                'default': '-2.', 'description': "''"},
                           {'name': 'angular_velocity_cov',         'default': '0.01', 'description': "''"},
                           {'name': 'linear_accel_cov',             'default': '0.01', 'description': "''"},
                           {'name': 'diagnostics_period',           'default': '0.0', 'description': 'Rate of publishing diagnostics. 0=Disabled'},
                           {'name': 'publish_tf',                   'default': 'true', 'description': '[bool] enable/disable publishing static & dynamic TF'},
                           {'name': 'tf_publish_rate',              'default': '0.0', 'description': '[double] rate in Hz for publishing dynamic TF'},
                           {'name': 'pointcloud.enable',            'default': 'false', 'description': ''},
                           {'name': 'pointcloud.stream_filter',     'default': '2', 'description': 'texture stream for pointcloud'},
                           {'name': 'pointcloud.stream_index_filter','default': '0', 'description': 'texture stream index for pointcloud'},
                           {'name': 'pointcloud.ordered_pc',        'default': 'false', 'description': ''},
                           {'name': 'pointcloud.allow_no_texture_points', 'default': 'false', 'description': "''"},
                           {'name': 'align_depth.enable',           'default': 'false', 'description': 'enable align depth filter'},
                           {'name': 'colorizer.enable',             'default': 'false', 'description': 'enable colorizer filter'},
                           {'name': 'decimation_filter.enable',     'default': 'false', 'description': 'enable_decimation_filter'},
                           {'name': 'spatial_filter.enable',        'default': 'false', 'description': 'enable_spatial_filter'},
                           {'name': 'temporal_filter.enable',       'default': 'false', 'description': 'enable_temporal_filter'},
                           {'name': 'disparity_filter.enable',      'default': 'false', 'description': 'enable_disparity_filter'},
                           {'name': 'hole_filling_filter.enable',   'default': 'false', 'description': 'enable_hole_filling_filter'},
                           {'name': 'hdr_merge.enable',             'default': 'false', 'description': 'hdr_merge filter enablement flag'},
                           {'name': 'wait_for_device_timeout',      'default': '-1.', 'description': 'Timeout for waiting for device to connect (Seconds)'},
                           {'name': 'reconnect_timeout',            'default': '6.', 'description': 'Timeout(seconds) between consequtive reconnection attempts'},
                          ]




with this subscriber from rs_launch 


import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped
import cv2
import numpy as np
from cv_bridge import CvBridge
from ultralytics import YOLO
import open3d as o3d 
from ament_index_python.packages import get_package_share_directory
import pyrealsense2 as rs
import os


# smart_robots_dir = get_package_share_directory("smart_robots_controller")
# model_file = os.path.join(smart_robots_dir, "config", "yolov8n.pt")
model_file = "/app/augmented-robots/server/nux_ws/src/smart_robots_controller/config/yolov8n.pt"

model = YOLO(model_file)
class ImageSubscriber(Node):

    def __init__(self):
        super().__init__('image_subscriber_yolo_tb3')
        self.image_sub = self.create_subscription(Image, '/camera/color/image_raw', self.image_callback, 10)
        self.depth_sub = self.create_subscription(PointStamped, 'image/zDepth', self.zDepth_callback, 10)

        self.bridge = CvBridge()

        timer_period = 0.5

        self.center_pub = self.create_publisher(PointStamped , '/image/center_coordinates' , 10)
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.i = 0
        self.center_coordinates = []


    

    def image_callback(self, msg):
        # global center_coordinates


        np_arr = np.frombuffer(msg.data, dtype=np.uint8)
        cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

        results = model(cv_image)
        results = model.predict(source=cv_image, show=False, stream=True, verbose=False) 

        for r in results:
            boxes = r.boxes
            for box in boxes:
            # bounding box
                #print(box.xyxy[0])
                x1, y1, x2, y2 = box.xyxy[0]
                confidence =box.conf[0]
                cls = int(box.cls[0])

                if model.names[cls] == "person" and confidence > 0.5 : # we can use door or window instead person !!!
                    
                    cv2.rectangle(cv_image , (int(x1),int(y1)) , (int(x2) , int(y2)) , (0,255,0) , 2)


                    (x_center,y_center) = (x2 + x1)/2, (y2+y1)/2 
                    center_coordinates = (int(x_center),int(y_center))

                    try:
                        
                        # zDepth = None  #depth_frame.get_distance(int(x_center),int(y_center))
                        print("Nothing just  ....." )

                    except (AttributeError , NameError):
                        print("Depth unavailable")

                    
                    # print(zDepth)

                    # frame_center = (cv_image.shape[1]//2 ,cv_image.shape[0]//2)
                    # error_x = center_coordinates[0] - frame_center[0]
                    # error_y = center_coordinates[1] - frame_center[1]

                    # self.center_coordinates.append(  (int(x_center) , int(y_center) ))

                    # print(f"Error: x ={error_x} , y={error_y}")

                    # center_msg = PointStamped()
                    # center_msg.header.stamp = self.get_clock().now().to_msg()
                    # center_msg.point.x = x_center
                    # center_msg.point.y = y_center
                    # self.center_pub.publish(center_msg)
                    # self.get_logger().info(f"Center coordinates : ({x_center} , {y_center})")
                    
                    if center_coordinates is not None :
            
                        cv2.circle(cv_image, center_coordinates, 5, (0,0,255), 5)
                
               

        cv2.imshow('RealSense Image subscriber', cv_image)
        cv2.waitKey(1)

    def timer_callback(self):


        if len(self.center_coordinates)>0 :

            x_center , y_center = self.center_coordinates[-1]

            msg = PointStamped()
            msg.header.stamp = self. get_clock().now().to_msg()
            msg.point.x = float(x_center)
            msg.point.y = float(y_center)

            self.center_pub.publish(msg)
            self.get_logger().info(f"Publishing center Coordinates: {msg.point.x} ,{msg.point.y}")
            self.i += 1
        else:
            self.get_logger().info('No center Coordinates:')

    def zDepth_callback(self ,msg):

        msg = PointStamped()
        msg.header.stamp = self.get_clock().now().to_msg()
        # msg.point.z = float(x_center)
        # x_center  =center_msg.point.x
        # y_center =center_msg.point.y
        zDepth  = msg.point.z

        self.get_logger().info(f"Publishing zDepth value: {zDepth}")
        

 



def main():
    rclpy.init()
    node = ImageSubscriber()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()


i get this erro 

WARNING ⚠️ 'source' is missing. Using 'source=/usr/local/lib/python3.10/dist-packages/ultralytics/assets'.

image 1/2 /usr/local/lib/python3.10/dist-packages/ultralytics/assets/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 130.4ms
image 2/2 /usr/local/lib/python3.10/dist-packages/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 130.2ms
Speed: 10.0ms preprocess, 130.3ms inference, 750.8ms postprocess per image at shape (1, 3, 384, 640)
WARNING ⚠️ 'source' is missing. Using 'source=/usr/local/lib/python3.10/dist-packages/ultralytics/assets'.
Nothing just  .....
Nothing just  .....
Nothing just  .....
Nothing just  .....
Nothing just  .....
Traceback (most recent call last):
  File "/app/augmented-robots/server/nux_ws/src/smart_robots_controller/smart_robots_controller/image_subscriber_yolo_tb3.py", line 143, in <module>
    main()
  File "/app/augmented-robots/server/nux_ws/src/smart_robots_controller/smart_robots_controller/image_subscriber_yolo_tb3.py", line 139, in main
    rclpy.spin(node)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/__init__.py", line 222, in spin
    executor.spin_once()
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 739, in spin_once
    self._spin_once_impl(timeout_sec)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 736, in _spin_once_impl
    raise handler.exception()
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/task.py", line 239, in __call__
    self._handler.send(None)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 437, in handler
    await call_coroutine(entity, arg)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 362, in _execute_subscription
    await await_or_execute(sub.callback, msg)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 107, in await_or_execute
    return callback(*args)
  File "/app/augmented-robots/server/nux_ws/src/smart_robots_controller/smart_robots_controller/image_subscriber_yolo_tb3.py", line 99, in image_callback
    cv2.imshow('RealSense Image subscriber', cv_image)
cv2.error: OpenCV(4.9.0) /io/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'












import rclpy
from rclpy.node import Node
from sensor_msgs.msg import CompressedImage
import cv2
import numpy as np

class CompressedImageSubscriber(Node):
    def __init__(self):
        super().__init__('compressed_image_subscriber')
        # Subscribe to the compressed image topic
        self.subscription = self.create_subscription(
            CompressedImage,
            '/new/camera/image_raw',
            self.image_callback,
            10
        )
        self.subscription  # prevent unused variable warning

    def image_callback(self, msg):
        # Convert the compressed image data to a format that OpenCV can use
        np_arr = np.frombuffer(msg.data, dtype=np.uint8)
        cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)  # Decode as color image

        # Display the image using OpenCV
        cv2.imshow('Compressed Image Viewer', cv_image)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            self.get_logger().info('Shutting down image viewer.')
            cv2.destroyAllWindows()
            rclpy.shutdown()

def main(args=None):
    rclpy.init(args=args)
    subscriber = CompressedImageSubscriber()
    try:
        rclpy.spin(subscriber)
    except KeyboardInterrupt:
        pass  # Handle Ctrl+C gracefully
    finally:
        subscriber.destroy_node()
        rclpy.shutdown()
        cv2.destroyAllWindows()  # Make sure to destroy all OpenCV windows after shutdown

if __name__ == '__main__':
    main()






import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CompressedImage
import cv2
import numpy as np
from cv_bridge import CvBridge

class ImageProcessor(Node):
    def __init__(self):
        super().__init__('image_processor')
        # Subscription to the uncompressed image topic
        self.subscription = self.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.image_callback,
            10
        )
        # Publisher for the compressed image topic
        self.publisher_ = self.create_publisher(
            CompressedImage,
            '/new/camera/image_raw',
            10
        )
        self.bridge = CvBridge()
        self.latest_image = None  # Store the latest image
        self.timer = self.create_timer(0.1, self.timer_callback)  # 10 Hz publishing rate

    def image_callback(self, msg):
        # Convert ROS Image message to OpenCV image
        self.latest_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

    def timer_callback(self):
        if self.latest_image is not None:
            # Compress the image to JPEG with reduced quality
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50]  # Quality range from 0 to 100 (lower means more compression)
            result, encimg = cv2.imencode('.jpg', self.latest_image, encode_param)
            if not result:
                raise RuntimeError("Could not encode image!")

            # Prepare the compressed image message
            compressed_msg = CompressedImage()
            compressed_msg.header.stamp = self.get_clock().now().to_msg()  # Set the timestamp to now
            compressed_msg.format = "jpeg"
            compressed_msg.data = np.array(encimg).tobytes()
            self.publisher_.publish(compressed_msg)
            self.get_logger().info('Published compressed image with reduced quality')

def main(args=None):
    rclpy.init(args=args)
    image_processor = ImageProcessor()
    try:
        rclpy.spin(image_processor)
    except KeyboardInterrupt:
        pass  # Handle Ctrl+C gracefully
    finally:
        image_processor.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()






this is the publisher that is workig good in another divice 


import rclpy 
from rclpy.node import Node 
from sensor_msgs.msg import Image 
from geometry_msgs.msg import PointStamped
from cv_bridge import CvBridge 
import cv2 
import pyrealsense2 as rs
import numpy as np
 
class ImagePublisher(Node):
 
  def __init__(self):
 
  
    super().__init__('image_publisher')

    self.publisher_ = self.create_publisher(Image, '/camera/image_raw/compressed', 10) 
    self.bridge = CvBridge()
    self.center_sub = self.create_subscription(PointStamped , '/image/center_coordinates' , self.center_callback ,10)
    self.depth_pub = self.create_publisher(PointStamped , '/image/zDepth' , 10)

    self.pipeline =  rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.depth , 640 ,480 , rs.format.z16 , 30)
    config.enable_stream(rs.stream.color , 640 ,480 , rs.format.bgr8 , 30) # FPS = 30 

    self.pipeline.start(config)

  
    self.timer_period = 0.1  
    self.timer = self.create_timer(self.timer_period , self.timer_callback)
    self.x_center = None
    self.y_center = None 

  def timer_callback(self):
    frames = self.pipeline.wait_for_frames()  
    depth_frame = frames.get_depth_frame()
    color_frame = frames.get_color_frame()
    depth_frame = frames.get_depth_frame()
    if color_frame:
      color_image = np.asanyarray(color_frame.get_data()) 
      #rgb_image = cv2.cvtColor(color_image , cv2.COLOR_BGR2RGB)
 
    
#      if self.center_sub:
#          if self.center_x is not None and self.center_y is not None:
#            print(f"Center Coordinates : ({self.center_x} , {self.center_y})")
   
      _ , compressed_image_data = cv2.imencode('.jpg' , color_image , [cv2.IMWRITE_JPEG_QUALITY , 20])
      compressed_image_data  = compressed_image_data.tobytes()
      
      msg = Image()
      msg.header.stamp = self.get_clock().now().to_msg()

      msg.data = compressed_image_data 
      msg.encoding = 'jpeg'

      self.publisher_ .publish(msg) 
#      self.timer_callback(self.x_center , self.y_center)
    if self.x_center is not None and self.y_center is not None:
      zDepth = depth_frame.get_distance(int(self.x_center),int(self.y_center))
      depth_msg = PointStamped()
      depth_msg.header.stamp = self.get_clock().now().to_msg()
      depth_msg.point.x = self.x_center
      depth_msg.point.y = self.y_center
      depth_msg.point.z = zDepth
      self.depth_pub.publish(depth_msg)
      self.get_logger().info(f"Depth is  : {zDepth}")
  
  def center_callback(self ,msg):
#                  
    self.x_center = msg.point.x
    self.y_center = msg.point.y
#    zDepth = depth_frame.get_distance(int(self.x_center),int(self.y_center))
#    self.center_callback(self.x_center ,self.y_center)
#    self.get_logger().info(f"Depth is  : {zDepth})")
#    self.get_logger().info('I heard: "%s"' % msg.zDepth)
   
def main(args=None):

  rclpy.init(args=args)
  

  image_publisher = ImagePublisher()

  rclpy.spin(image_publisher)
  
  rclpy.shutdown()
  
if __name__ == '__main__':
  main()


and this is the subscriber to this  



import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped
import cv2
import numpy as np
from cv_bridge import CvBridge
from ultralytics import YOLO
import open3d as o3d 
from ament_index_python.packages import get_package_share_directory
import pyrealsense2 as rs
import os


# smart_robots_dir = get_package_share_directory("smart_robots_controller")
# model_file = os.path.join(smart_robots_dir, "config", "yolov8n.pt")
model_file = "/app/augmented-robots/server/nux_ws/src/smart_robots_controller/config/yolov8n.pt"

model = YOLO(model_file)
class ImageSubscriber(Node):

    def __init__(self):
        super().__init__('image_subscriber_yolo')
        self.image_sub = self.create_subscription(Image, '/camera/image_raw/compressed', self.image_callback, 10)
        self.depth_sub = self.create_subscription(PointStamped, 'image/zDepth', self.zDepth_callback, 10)

        self.bridge = CvBridge()

        timer_period = 0.5

        self.center_pub = self.create_publisher(PointStamped , '/image/center_coordinates' , 10)
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.i = 0
        self.center_coordinates = []


    

    def image_callback(self, msg):
        # global center_coordinates


        decoded_data = np.frombuffer(msg.data, dtype=np.uint8)
        decoded_image = cv2.imdecode(decoded_data, cv2.IMREAD_COLOR) 

        rbg_image = cv2.cvtColor(decoded_image , cv2.COLOR_BGR2RGB)

        results = model(rbg_image)
        results = model.predict(source=rbg_image, show=False, stream=True, verbose=False) 

        for r in results:
            boxes = r.boxes
            for box in boxes:
            # bounding box
                #print(box.xyxy[0])
                x1, y1, x2, y2 = box.xyxy[0]
                confidence =box.conf[0]
                cls = int(box.cls[0])

                if model.names[cls] == "person" and confidence > 0.5 : # we can use door or window instead person !!!
                    
                    cv2.rectangle(decoded_image , (int(x1),int(y1)) , (int(x2) , int(y2)) , (0,255,0) , 2)


                    (x_center,y_center) = (x2 + x1)/2, (y2+y1)/2 
                    center_coordinates = (int(x_center),int(y_center))

                    try:
                        
                        # zDepth = None  #depth_frame.get_distance(int(x_center),int(y_center))
                        print("Nothing just  ....." )

                    except (AttributeError , NameError):
                        print("Depth unavailable")

                    
                    # print(zDepth)

                    frame_center = (rbg_image.shape[1]//2 ,rbg_image.shape[0]//2)
                    error_x = center_coordinates[0] - frame_center[0]
                    error_y = center_coordinates[1] - frame_center[1]

                    self.center_coordinates.append(  (int(x_center) , int(y_center) ))

                    # print(f"Error: x ={error_x} , y={error_y}")

                    # center_msg = PointStamped()
                    # center_msg.header.stamp = self.get_clock().now().to_msg()
                    # center_msg.point.x = x_center
                    # center_msg.point.y = y_center
                    # self.center_pub.publish(center_msg)
                    # self.get_logger().info(f"Center coordinates : ({x_center} , {y_center})")
                    
                    if center_coordinates is not None :
            
                        cv2.circle(decoded_image, center_coordinates, 5, (0,0,255), 5)
                
               

        cv2.imshow('RealSense Image subscriber', decoded_image)
        cv2.waitKey(1)

    def timer_callback(self):


        if len(self.center_coordinates)>0 :

            x_center , y_center = self.center_coordinates[-1]

            msg = PointStamped()
            msg.header.stamp = self. get_clock().now().to_msg()
            msg.point.x = float(x_center)
            msg.point.y = float(y_center)

            self.center_pub.publish(msg)
            self.get_logger().info(f"Publishing center Coordinates: {msg.point.x} ,{msg.point.y}")
            self.i += 1
        else:
            self.get_logger().info('No center Coordinates:')

    def zDepth_callback(self ,msg):

        msg = PointStamped()
        msg.header.stamp = self.get_clock().now().to_msg()
        # msg.point.z = float(x_center)
        # x_center  =center_msg.point.x
        # y_center =center_msg.point.y
        zDepth  = msg.point.z

        self.get_logger().info(f"Publishing zDepth value: {zDepth}")
        

 



def main():
    rclpy.init()
    node = ImageSubscriber()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()

in raspberry pi i launch the file frome realsense camera rs_launch.py 
and i subscribe to /camera/image_raw with this 



import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image , CompressedImage
import cv2
import numpy as np
from cv_bridge import CvBridge
from ultralytics import YOLO  # Ensure YOLOv8 or compatible version is installed
import pyrealsense2 as rs

model_file = "/app/augmented-robots/server/nux_ws/src/smart_robots_controller/config/yolov8n.pt"

model = YOLO(model_file)

class ImageSubscriberYOLO(Node):
    def __init__(self):
        super().__init__('image_subscriber_yolo_tb3')
        self.subscription = self.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.image_callback,
            10
        )
        self.bridge = CvBridge()
        self.yolo_model = model  # Update path to your YOLO model file

    def image_callback(self, msg):
        # Convert ROS Image message to OpenCV image
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Process the image with YOLO
        results = self.yolo_model(cv_image)

        # Render detections
        cv_image = self.render_detections(cv_image, results)

        # Display the image using OpenCV
        cv2.imshow('YOLO Detections', cv_image)
        cv2.waitKey(1)

    def render_detections(self, image, results):
        # Draw bounding boxes and labels on the image
        for r in results:
            boxes = r.boxes
            for box in boxes:
            # bounding box
                #print(box.xyxy[0])
                x1, y1, x2, y2 = box.xyxy[0]
                confidence =box.conf[0]
                cls = int(box.cls[0])

                if model.names[cls] == "person" and confidence > 0.5 : # we can use door or window instead person !!!
                    
                    cv2.rectangle(image , (int(x1),int(y1)) , (int(x2) , int(y2)) , (0,255,0) , 2)


                    (x_center,y_center) = (x2 + x1)/2, (y2+y1)/2 
                    center_coordinates = (int(x_center),int(y_center))

                    try:
                        
                        # zDepth = None  #depth_frame.get_distance(int(x_center),int(y_center))
                        print("Nothing just  ....." )

                    except (AttributeError , NameError):
                        print("Depth unavailable")

                    
                    # print(zDepth)

                    frame_center = (image.shape[1]//2 ,image.shape[0]//2)
                    error_x = center_coordinates[0] - frame_center[0]
                    error_y = center_coordinates[1] - frame_center[1]

                    self.center_coordinates.append(  (int(x_center) , int(y_center) ))

                    # print(f"Error: x ={error_x} , y={error_y}")

                    # center_msg = PointStamped()
                    # center_msg.header.stamp = self.get_clock().now().to_msg()
                    # center_msg.point.x = x_center
                    # center_msg.point.y = y_center
                    # self.center_pub.publish(center_msg)
                    # self.get_logger().info(f"Center coordinates : ({x_center} , {y_center})")
                    
                    if center_coordinates is not None :
            
                        cv2.circle(image, center_coordinates, 5, (0,0,255), 5)
                
               

        cv2.imshow('RealSense Image subscriber', image)
        cv2.waitKey(1)

def main(args=None):
    rclpy.init(args=args)
    node = ImageSubscriberYOLO()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass  # Handle Ctrl+C gracefully
    finally:
        node.destroy_node()
        rclpy.shutdown()
        cv2.destroyAllWindows()

if __name__ == '__main__':
    main()

what i want is to create a node containe a subscriber to /camera/color/image_raw from rs_launch in the same node create a publisher with a compressed image and resduce the qualite in the topic 'new/camera/image_raw you can forget the part whene we calculate the depth and center cordinate 




import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CompressedImage
import cv2
import numpy as np
from cv_bridge import CvBridge

class ImageProcessor(Node):
    def __init__(self):
        super().__init__('image_processor')
        # Subscribe to the uncompressed image topic
        self.subscription = self.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.image_callback,
            10
        )
        self.publisher_ = self.create_publisher(
            CompressedImage,
            '/new/camera/image_raw',
            10
        )
        self.bridge = CvBridge()

    def image_callback(self, msg):
        # Convert ROS Image message to OpenCV image
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Compress the image to JPEG with reduced quality
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 50]  # Quality range from 0 to 100 (lower means more compression and less quality)
        result, encimg = cv2.imencode('.jpg', cv_image, encode_param)
        if not result:
            raise RuntimeError("Could not encode image!")

        # Prepare and publish the compressed image
        compressed_msg = CompressedImage()
        compressed_msg.header = msg.header  # Maintain timestamp and frame id
        compressed_msg.format = "jpeg"
        compressed_msg.data = np.array(encimg).tobytes()
        self.publisher_.publish(compressed_msg)
        self.get_logger().info('Published compressed image with reduced quality')

def main(args=None):
    rclpy.init(args=args)
    image_processor = ImageProcessor()
    try:
        rclpy.spin(image_processor)
    except KeyboardInterrupt:
        pass  # Handle Ctrl+C gracefully
    finally:
        image_processor.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()











import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
import cv2
import numpy as np
from cv_bridge import CvBridge
from ultralytics import YOLO  # Ensure YOLOv8 or compatible version is installed

class ImageSubscriberYOLO(Node):
    def __init__(self):
        super().__init__('image_subscriber_yolo')
        self.subscription = self.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.image_callback,
            10
        )
        self.bridge = CvBridge()
        self.yolo_model = YOLO('/path/to/yolov8n.pt')  # Update path to your YOLO model file

    def image_callback(self, msg):
        # Convert ROS Image message to OpenCV image
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Process the image with YOLO
        results = self.yolo_model(cv_image)

        # Render detections
        cv_image = self.render_detections(cv_image, results)

        # Display the image using OpenCV
        cv2.imshow('YOLO Detections', cv_image)
        cv2.waitKey(1)

    def render_detections(self, image, results):
        # Draw bounding boxes and labels on the image
        for result in results.xyxy[0]:  # results.xyxy[0] contains bbox coordinates
            if result[-1] > 0.3:  # Confidence threshold
                x1, y1, x2, y2, conf, cls_id = result
                cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)
                cv2.putText(image, f'{self.yolo_model.names[int(cls_id)]} {conf:.2f}', 
                            (int(x1), int(y1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)
        return image

def main(args=None):
    rclpy.init(args=args)
    node = ImageSubscriberYOLO()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass  # Handle Ctrl+C gracefully
    finally:
        node.destroy_node()
        rclpy.shutdown()
        cv2.destroyAllWindows()

if __name__ == '__main__':
    main()



import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import pyrealsense2 as rs
import numpy as np

class ImagePublisher(Node):
    def __init__(self):
        super().__init__('image_publisher')
        self.publisher_ = self.create_publisher(Image, '/camera/image_raw', 10)
        self.bridge = CvBridge()

        # Initialize the camera
        self.pipeline = rs.pipeline()
        config = rs.config()

        # Match the configuration with rs_launch parameters
        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)  # Depth stream
        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)  # Color stream

        # Start the camera pipeline with a context manager to handle exceptions
        try:
            self.pipeline.start(config)
            self.get_logger().info("RealSense camera started successfully.")
        except Exception as e:
            self.get_logger().error(f"Failed to start the camera pipeline: {str(e)}")
            return  # Stop initialization if the camera fails to start

        self.timer_period = 0.1  # seconds
        self.timer = self.create_timer(self.timer_period, self.timer_callback)

    def timer_callback(self):
        try:
            frames = self.pipeline.wait_for_frames()
            depth_frame = frames.get_depth_frame()
            color_frame = frames.get_color_frame()
        except Exception as e:
            self.get_logger().error(f"Failed to get frames: {str(e)}")
            return

        if not depth_frame or not color_frame:
            self.get_logger().warn("Incomplete frames received")
            return

        # Convert color frame to ROS Image message and publish
        color_image = np.asanyarray(color_frame.get_data())
        msg = self.bridge.cv2_to_imgmsg(color_image, encoding="bgr8")
        msg.header.stamp = self.get_clock().now().to_msg()
        self.publisher_.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    image_publisher = ImagePublisher()
    try:
        rclpy.spin(image_publisher)
    finally:
        image_publisher.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()



import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import pyrealsense2 as rs
import numpy as np

class ImagePublisher(Node):
    def __init__(self):
        super().__init__('image_publisher')
        self.publisher_ = self.create_publisher(Image, '/camera/image_raw', 10)
        self.bridge = CvBridge()

        # Initialize the camera
        self.pipeline = rs.pipeline()
        config = rs.config()

        # Configure the RealSense to use a lower resolution for color and depth to ensure compatibility
        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 15)  # Lower FPS to 15
        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 15)

        # Start the camera pipeline
        try:
            self.pipeline.start(config)
            self.get_logger().info("RealSense camera started successfully.")
        except Exception as e:
            self.get_logger().error(f"Failed to start the camera pipeline: {str(e)}")
        
        self.timer_period = 0.1  # seconds (10Hz)
        self.timer = self.create_timer(self.timer_period, self.timer_callback)

    def timer_callback(self):
        frames = None
        try:
            frames = self.pipeline.wait_for_frames(5000)  # Wait for a frame for up to 5000 ms
        except Exception as e:
            self.get_logger().error(f"Failed to get frames: {str(e)}")
            return

        depth_frame = frames.get_depth_frame()
        color_frame = frames.get_color_frame()

        if not depth_frame or not color_frame:
            self.get_logger().warn("No frames data received")
            return

        # Process color frame
        color_image = np.asanyarray(color_frame.get_data())
        msg = self.bridge.cv2_to_imgmsg(color_image, encoding="bgr8")
        msg.header.stamp = self.get_clock().now().to_msg()
        self.publisher_.publish(msg)
        self.get_logger().info("Published a new image.")

def main(args=None):
    rclpy.init(args=args)
    image_publisher = ImagePublisher()
    rclpy.spin(image_publisher)
    image_publisher.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()





import rclpy
from rclpy.node import Node
from sensor_msgs.msg import CompressedImage
import cv2
import numpy as np
from cv_bridge import CvBridge

class SimpleImageSubscriber(Node):
    def __init__(self):
        super().__init__('simple_image_subscriber')
        self.subscription = self.create_subscription(
            CompressedImage,
            '/camera/image_raw/compressed',
            self.image_callback,
            10
        )
        self.bridge = CvBridge()

    def image_callback(self, msg):
        try:
            # Convert the compressed image ROS message to a format that OpenCV can use
            cv_image = self.bridge.compressed_imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Display the image using OpenCV
            cv2.imshow('Camera Image', cv_image)
            cv2.waitKey(1)  # Wait for a key press for 1 millisecond

        except Exception as e:
            self.get_logger().error('Failed to convert image: %s' % str(e))

def main(args=None):
    rclpy.init(args=args)
    node = SimpleImageSubscriber()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()
        cv2.destroyAllWindows()  # Make sure to destroy all OpenCV windows

if __name__ == '__main__':
    main()



import rclpy 
from rclpy.node import Node 
from sensor_msgs.msg import Image 
from geometry_msgs.msg import PointStamped
from cv_bridge import CvBridge 
import cv2 
import pyrealsense2 as rs
import numpy as np
 
class ImagePublisher(Node):
 
  def __init__(self):
 
  
    super().__init__('image_publisher')

    self.publisher_ = self.create_publisher(Image, '/camera/image_raw/compressed', 10) 
    self.bridge = CvBridge()
    self.center_sub = self.create_subscription(PointStamped , '/image/center_coordinates' , self.center_callback ,10)
    self.depth_pub = self.create_publisher(PointStamped , '/image/zDepth' , 10)

    self.pipeline =  rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.depth , 640 ,480 , rs.format.z16 , 30)
    config.enable_stream(rs.stream.color , 640 ,480 , rs.format.bgr8 , 30) # FPS = 30 

    self.pipeline.start(config)

  
    self.timer_period = 0.1  
    self.timer = self.create_timer(self.timer_period , self.timer_callback)
    self.x_center = None
    self.y_center = None 

  def timer_callback(self):
    frames = self.pipeline.wait_for_frames()  
    depth_frame = frames.get_depth_frame()
    color_frame = frames.get_color_frame()
    depth_frame = frames.get_depth_frame()
    if color_frame:
      color_image = np.asanyarray(color_frame.get_data()) 
      #rgb_image = cv2.cvtColor(color_image , cv2.COLOR_BGR2RGB)
 
    
#      if self.center_sub:
#          if self.center_x is not None and self.center_y is not None:
#            print(f"Center Coordinates : ({self.center_x} , {self.center_y})")
   
      _ , compressed_image_data = cv2.imencode('.jpg' , color_image , [cv2.IMWRITE_JPEG_QUALITY , 20])
      compressed_image_data  = compressed_image_data.tobytes()
      
      msg = Image()
      msg.header.stamp = self.get_clock().now().to_msg()

      msg.data = compressed_image_data 
      msg.encoding = 'jpeg'

      self.publisher_ .publish(msg) 
#      self.timer_callback(self.x_center , self.y_center)
    if self.x_center is not None and self.y_center is not None:
      zDepth = depth_frame.get_distance(int(self.x_center),int(self.y_center))
      depth_msg = PointStamped()
      depth_msg.header.stamp = self.get_clock().now().to_msg()
      depth_msg.point.x = self.x_center
      depth_msg.point.y = self.y_center
      depth_msg.point.z = zDepth
      self.depth_pub.publish(depth_msg)
      self.get_logger().info(f"Depth is  : {zDepth}")
  
  def center_callback(self ,msg):
#                  
    self.x_center = msg.point.x
    self.y_center = msg.point.y
#    zDepth = depth_frame.get_distance(int(self.x_center),int(self.y_center))
#    self.center_callback(self.x_center ,self.y_center)
#    self.get_logger().info(f"Depth is  : {zDepth})")
#    self.get_logger().info('I heard: "%s"' % msg.zDepth)
   
def main(args=None):

  rclpy.init(args=args)
  

  image_publisher = ImagePublisher()

  rclpy.spin(image_publisher)
  
  rclpy.shutdown()
  
if __name__ == '__main__':
  main()






import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped
import cv2
import numpy as np
from cv_bridge import CvBridge
from ultralytics import YOLO
import open3d as o3d 
from ament_index_python.packages import get_package_share_directory
import pyrealsense2 as rs
import os


# smart_robots_dir = get_package_share_directory("smart_robots_controller")
# model_file = os.path.join(smart_robots_dir, "config", "yolov8n.pt")
model_file = "/app/augmented-robots/server/nux_ws/src/smart_robots_controller/config/yolov8n.pt"

model = YOLO(model_file)
class ImageSubscriber(Node):

    def __init__(self):
        super().__init__('image_subscriber_yolo')
        self.image_sub = self.create_subscription(Image, '/camera/image_raw/compressed', self.image_callback, 10)
        self.depth_sub = self.create_subscription(PointStamped, 'image/zDepth', self.zDepth_callback, 10)

        self.bridge = CvBridge()

        timer_period = 0.5

        self.center_pub = self.create_publisher(PointStamped , '/image/center_coordinates' , 10)
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.i = 0
        self.center_coordinates = []


    

    def image_callback(self, msg):
        # global center_coordinates


        decoded_data = np.frombuffer(msg.data, dtype=np.uint8)
        decoded_image = cv2.imdecode(decoded_data, cv2.IMREAD_COLOR) 

        rbg_image = cv2.cvtColor(decoded_image , cv2.COLOR_BGR2RGB)

        results = model(rbg_image)
        results = model.predict(source=rbg_image, show=False, stream=True, verbose=False) 

        for r in results:
            boxes = r.boxes
            for box in boxes:
            # bounding box
                #print(box.xyxy[0])
                x1, y1, x2, y2 = box.xyxy[0]
                confidence =box.conf[0]
                cls = int(box.cls[0])

                if model.names[cls] == "person" and confidence > 0.5 : # we can use door or window instead person !!!
                    
                    cv2.rectangle(decoded_image , (int(x1),int(y1)) , (int(x2) , int(y2)) , (0,255,0) , 2)


                    (x_center,y_center) = (x2 + x1)/2, (y2+y1)/2 
                    center_coordinates = (int(x_center),int(y_center))

                    try:
                        
                        # zDepth = None  #depth_frame.get_distance(int(x_center),int(y_center))
                        print("Nothing just  ....." )

                    except (AttributeError , NameError):
                        print("Depth unavailable")

                    
                    # print(zDepth)

                    frame_center = (rbg_image.shape[1]//2 ,rbg_image.shape[0]//2)
                    error_x = center_coordinates[0] - frame_center[0]
                    error_y = center_coordinates[1] - frame_center[1]

                    self.center_coordinates.append(  (int(x_center) , int(y_center) ))

                    # print(f"Error: x ={error_x} , y={error_y}")

                    # center_msg = PointStamped()
                    # center_msg.header.stamp = self.get_clock().now().to_msg()
                    # center_msg.point.x = x_center
                    # center_msg.point.y = y_center
                    # self.center_pub.publish(center_msg)
                    # self.get_logger().info(f"Center coordinates : ({x_center} , {y_center})")
                    
                    if center_coordinates is not None :
            
                        cv2.circle(decoded_image, center_coordinates, 5, (0,0,255), 5)
                
               

        cv2.imshow('RealSense Image subscriber', decoded_image)
        cv2.waitKey(1)

    def timer_callback(self):


        if len(self.center_coordinates)>0 :

            x_center , y_center = self.center_coordinates[-1]

            msg = PointStamped()
            msg.header.stamp = self. get_clock().now().to_msg()
            msg.point.x = float(x_center)
            msg.point.y = float(y_center)

            self.center_pub.publish(msg)
            self.get_logger().info(f"Publishing center Coordinates: {msg.point.x} ,{msg.point.y}")
            self.i += 1
        else:
            self.get_logger().info('No center Coordinates:')

    def zDepth_callback(self ,msg):

        msg = PointStamped()
        msg.header.stamp = self.get_clock().now().to_msg()
        # msg.point.z = float(x_center)
        # x_center  =center_msg.point.x
        # y_center =center_msg.point.y
        zDepth  = msg.point.z

        self.get_logger().info(f"Publishing zDepth value: {zDepth}")
        

 



def main():
    rclpy.init()
    node = ImageSubscriber()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()
